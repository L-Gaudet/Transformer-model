{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer from Attention Is All You Need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /Users/fsl/anaconda3/lib/python3.10/site-packages (2.0.0)\n",
      "Requirement already satisfied: filelock in /Users/fsl/anaconda3/lib/python3.10/site-packages (from torch) (3.9.0)\n",
      "Requirement already satisfied: networkx in /Users/fsl/anaconda3/lib/python3.10/site-packages (from torch) (3.1)\n",
      "Requirement already satisfied: jinja2 in /Users/fsl/anaconda3/lib/python3.10/site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: typing-extensions in /Users/fsl/anaconda3/lib/python3.10/site-packages (from torch) (4.4.0)\n",
      "Requirement already satisfied: sympy in /Users/fsl/anaconda3/lib/python3.10/site-packages (from torch) (1.11.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/fsl/anaconda3/lib/python3.10/site-packages (from jinja2->torch) (2.1.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in /Users/fsl/anaconda3/lib/python3.10/site-packages (from sympy->torch) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: torchtext==0.6.0 in /Users/fsl/anaconda3/lib/python3.10/site-packages (0.6.0)\n",
      "Requirement already satisfied: tqdm in /Users/fsl/anaconda3/lib/python3.10/site-packages (from torchtext==0.6.0) (4.65.0)\n",
      "Requirement already satisfied: sentencepiece in /Users/fsl/anaconda3/lib/python3.10/site-packages (from torchtext==0.6.0) (0.1.98)\n",
      "Requirement already satisfied: six in /Users/fsl/anaconda3/lib/python3.10/site-packages (from torchtext==0.6.0) (1.16.0)\n",
      "Requirement already satisfied: numpy in /Users/fsl/anaconda3/lib/python3.10/site-packages (from torchtext==0.6.0) (1.24.2)\n",
      "Requirement already satisfied: torch in /Users/fsl/anaconda3/lib/python3.10/site-packages (from torchtext==0.6.0) (2.0.0)\n",
      "Requirement already satisfied: requests in /Users/fsl/anaconda3/lib/python3.10/site-packages (from torchtext==0.6.0) (2.28.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/fsl/anaconda3/lib/python3.10/site-packages (from requests->torchtext==0.6.0) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/fsl/anaconda3/lib/python3.10/site-packages (from requests->torchtext==0.6.0) (1.26.15)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /Users/fsl/anaconda3/lib/python3.10/site-packages (from requests->torchtext==0.6.0) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/fsl/anaconda3/lib/python3.10/site-packages (from requests->torchtext==0.6.0) (2022.12.7)\n",
      "Requirement already satisfied: typing-extensions in /Users/fsl/anaconda3/lib/python3.10/site-packages (from torch->torchtext==0.6.0) (4.4.0)\n",
      "Requirement already satisfied: sympy in /Users/fsl/anaconda3/lib/python3.10/site-packages (from torch->torchtext==0.6.0) (1.11.1)\n",
      "Requirement already satisfied: jinja2 in /Users/fsl/anaconda3/lib/python3.10/site-packages (from torch->torchtext==0.6.0) (3.1.2)\n",
      "Requirement already satisfied: networkx in /Users/fsl/anaconda3/lib/python3.10/site-packages (from torch->torchtext==0.6.0) (3.1)\n",
      "Requirement already satisfied: filelock in /Users/fsl/anaconda3/lib/python3.10/site-packages (from torch->torchtext==0.6.0) (3.9.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/fsl/anaconda3/lib/python3.10/site-packages (from jinja2->torch->torchtext==0.6.0) (2.1.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in /Users/fsl/anaconda3/lib/python3.10/site-packages (from sympy->torch->torchtext==0.6.0) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: spacy in /Users/fsl/anaconda3/lib/python3.10/site-packages (3.5.2)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /Users/fsl/anaconda3/lib/python3.10/site-packages (from spacy) (8.1.9)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /Users/fsl/anaconda3/lib/python3.10/site-packages (from spacy) (4.65.0)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /Users/fsl/anaconda3/lib/python3.10/site-packages (from spacy) (3.3.0)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /Users/fsl/anaconda3/lib/python3.10/site-packages (from spacy) (1.24.2)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /Users/fsl/anaconda3/lib/python3.10/site-packages (from spacy) (6.3.0)\n",
      "Requirement already satisfied: jinja2 in /Users/fsl/anaconda3/lib/python3.10/site-packages (from spacy) (3.1.2)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /Users/fsl/anaconda3/lib/python3.10/site-packages (from spacy) (1.0.4)\n",
      "Requirement already satisfied: setuptools in /Users/fsl/anaconda3/lib/python3.10/site-packages (from spacy) (65.6.3)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /Users/fsl/anaconda3/lib/python3.10/site-packages (from spacy) (3.0.8)\n",
      "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /Users/fsl/anaconda3/lib/python3.10/site-packages (from spacy) (0.7.0)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /Users/fsl/anaconda3/lib/python3.10/site-packages (from spacy) (1.0.9)\n",
      "Requirement already satisfied: pathy>=0.10.0 in /Users/fsl/anaconda3/lib/python3.10/site-packages (from spacy) (0.10.1)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /Users/fsl/anaconda3/lib/python3.10/site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /Users/fsl/anaconda3/lib/python3.10/site-packages (from spacy) (1.10.7)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /Users/fsl/anaconda3/lib/python3.10/site-packages (from spacy) (1.1.1)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /Users/fsl/anaconda3/lib/python3.10/site-packages (from spacy) (2.0.7)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /Users/fsl/anaconda3/lib/python3.10/site-packages (from spacy) (2.4.6)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /Users/fsl/anaconda3/lib/python3.10/site-packages (from spacy) (2.28.1)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /Users/fsl/anaconda3/lib/python3.10/site-packages (from spacy) (2.0.8)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/fsl/anaconda3/lib/python3.10/site-packages (from spacy) (23.0)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /Users/fsl/anaconda3/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy) (4.4.0)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /Users/fsl/anaconda3/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/fsl/anaconda3/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.15)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/fsl/anaconda3/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2022.12.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/fsl/anaconda3/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /Users/fsl/anaconda3/lib/python3.10/site-packages (from thinc<8.2.0,>=8.1.8->spacy) (0.7.9)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /Users/fsl/anaconda3/lib/python3.10/site-packages (from thinc<8.2.0,>=8.1.8->spacy) (0.0.4)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /Users/fsl/anaconda3/lib/python3.10/site-packages (from typer<0.8.0,>=0.3.0->spacy) (8.0.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/fsl/anaconda3/lib/python3.10/site-packages (from jinja2->spacy) (2.1.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting en-core-web-sm==3.5.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.5.0/en_core_web_sm-3.5.0-py3-none-any.whl (12.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: spacy<3.6.0,>=3.5.0 in /Users/fsl/anaconda3/lib/python3.10/site-packages (from en-core-web-sm==3.5.0) (3.5.2)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /Users/fsl/anaconda3/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.0.7)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /Users/fsl/anaconda3/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (8.1.9)\n",
      "Requirement already satisfied: pathy>=0.10.0 in /Users/fsl/anaconda3/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.10.1)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /Users/fsl/anaconda3/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.10.7)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /Users/fsl/anaconda3/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (4.65.0)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /Users/fsl/anaconda3/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.1.1)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /Users/fsl/anaconda3/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.28.1)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /Users/fsl/anaconda3/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.0.12)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /Users/fsl/anaconda3/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.0.8)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /Users/fsl/anaconda3/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.0.9)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /Users/fsl/anaconda3/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.0.8)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/fsl/anaconda3/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (23.0)\n",
      "Requirement already satisfied: setuptools in /Users/fsl/anaconda3/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (65.6.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /Users/fsl/anaconda3/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.4.6)\n",
      "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /Users/fsl/anaconda3/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.7.0)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /Users/fsl/anaconda3/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.0.4)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /Users/fsl/anaconda3/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.3.0)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /Users/fsl/anaconda3/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.24.2)\n",
      "Requirement already satisfied: jinja2 in /Users/fsl/anaconda3/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.1.2)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /Users/fsl/anaconda3/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (6.3.0)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /Users/fsl/anaconda3/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (4.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/fsl/anaconda3/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2022.12.7)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /Users/fsl/anaconda3/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/fsl/anaconda3/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.26.15)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/fsl/anaconda3/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.4)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /Users/fsl/anaconda3/lib/python3.10/site-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.7.9)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /Users/fsl/anaconda3/lib/python3.10/site-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.0.4)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /Users/fsl/anaconda3/lib/python3.10/site-packages (from typer<0.8.0,>=0.3.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (8.0.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/fsl/anaconda3/lib/python3.10/site-packages (from jinja2->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.1.1)\n",
      "Installing collected packages: en-core-web-sm\n",
      "Successfully installed en-core-web-sm-3.5.0\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "# import necessary packages\n",
    "%pip install torch\n",
    "%pip install torchtext==0.6.0\n",
    "%pip install -U spacy\n",
    "import torch\n",
    "import torch.nn as nn \n",
    "!python -m spacy download en_core_web_sm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention\n",
    "\n",
    "#### See section 3.2 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll start by building the self attention module, this is what makes the transformer architecture so powerfull. It takes in an embedding size, and a number of heads. We split the embedding into however many heads there are, this is multi-head attention. \n",
    "\n",
    "(ex. if we have an embedding size 256 and heads is 8, we split it into 8x32 parts)\n",
    "\n",
    "An attention layer is a component within neural network architectures that enables the model to focus on specific parts of the input data while processing it. The attention mechanism assigns different weights to the input elements based on their relative importance for the given context. This allows the model to selectively focus on relevant parts of the input when making predictions or generating outputs. Here's a high-level description of how an attention layer works in the context of the self-attention mechanism used in the Transformer architecture:\n",
    "\n",
    "- Input: The attention layer takes three inputs: queries (Q), keys (K), and values (V). In the case of self-attention, all three of these inputs are derived from the same input sequence (e.g., a sentence in a natural language processing task). The input sequence is first converted into continuous embeddings.\n",
    "\n",
    "- Linear Projections: The input embeddings are passed through separate linear layers to create query, key, and value matrices. These linear layers learn to project the input embeddings into different subspaces that capture different aspects of the data.\n",
    "\n",
    "- Dot Product Attention: The dot product between the query matrix (Q) and the transpose of the key matrix (K) is computed. This results in an attention score matrix that measures the compatibility between each pair of query and key elements. The intuition behind this is that the model learns to attend to input elements with higher compatibility scores.\n",
    "\n",
    "- Scaling: The dot product can result in large values, which can cause gradients to become too small when backpropagating through the softmax function. To mitigate this issue, the dot product attention scores are scaled by dividing them by the square root of the key's dimensionality.\n",
    "\n",
    "- Masking (optional): In some cases, such as when processing input sequences in the decoder of a Transformer, certain positions should not be attended to (e.g., future positions). A mask can be applied to the scaled dot product attention scores to prevent the model from attending to these positions.\n",
    "\n",
    "- Softmax: A softmax function is applied to the scaled (and masked) attention scores along the key dimension. This normalizes the scores, so they sum up to 1, producing the final attention weights.\n",
    "\n",
    "- Weighted Sum: The attention weights are multiplied by the value matrix (V) to compute a weighted sum. This step essentially aggregates the values based on their computed attention weights, producing the final output of the attention layer.\n",
    "\n",
    "- Output: The output of the attention layer is a continuous vector representation that combines the input elements based on their relative importance. This output can be further processed by the model or used as input for subsequent layers.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "  def __init__(self, embed_size, heads):\n",
    "    super(SelfAttention, self).__init__()\n",
    "\n",
    "    assert (embed_size % heads == 0), 'Embed size not divisible by heads'\n",
    "\n",
    "    self.embed_size = embed_size\n",
    "    self.heads = heads\n",
    "    self.head_dimension = embed_size // heads\n",
    "\n",
    "    # define linear layers to send queries, keys, and values through\n",
    "    self.values = nn.Linear(self.head_dimension, self.head_dimension, bias=False)\n",
    "    self.keys = nn.Linear(self.head_dimension, self.head_dimension, bias=False)\n",
    "    self.queries = nn.Linear(self.head_dimension, self.head_dimension, bias=False)\n",
    "    \n",
    "    # concatenate heads after multi-head attention\n",
    "    self.fully_connected_out = nn.Linear(embed_size, embed_size)\n",
    "\n",
    "  def forward(self, values, keys, query, mask):\n",
    "    # num training examples to send in at one time\n",
    "    N = query.shape[0]\n",
    "\n",
    "    # these correspond to source sentence length and target sentence length\n",
    "    value_len = values.shape[1]\n",
    "    key_len = keys.shape[1]\n",
    "    query_len = query.shape[1]\n",
    "\n",
    "    # split embeddings into multiple heads\n",
    "    values = values.reshape(N, value_len, self.heads, self.head_dimension)\n",
    "    keys = keys.reshape(N, key_len, self.heads, self.head_dimension)\n",
    "    queries = query.reshape(N, query_len, self.heads, self.head_dimension)\n",
    "\n",
    "    # send through linear layers\n",
    "    values = self.values(values)\n",
    "    keys = self.keys(keys)\n",
    "    queries = self.queries(queries)\n",
    "\n",
    "    #------- MatMul Q and K(Transposed) ---------#\n",
    "      # queries shape: (N, query_len, heads, head_dimension)\n",
    "      # keys shape: (N, key_len, heads, head_dimension)\n",
    "    # we want\n",
    "      # QdotK shape: (N, heads, query_len, key_len)\n",
    "    QdotKt = torch.einsum('nqhd,nkhd->nhqk', [queries, keys])\n",
    "\n",
    "    #------------ Scale ------------#\n",
    "    # QdotKt = QdotKt / (self.embed_size ** (1/2))\n",
    "\n",
    "    #----- Mask (for decoder) ------#\n",
    "    # decoder requires masked multi-head attention\n",
    "    if mask is not None:\n",
    "      # closes elements above the diagonal so that the model can't see future values\n",
    "      QdotKt = QdotKt.masked_fill(mask == 0, float('-1e20'))\n",
    "\n",
    "    #---------- Softmax ------------#\n",
    "    soft = torch.softmax(QdotKt / (self.embed_size ** (1/2)), dim=3)\n",
    "    # attention = torch.softmax(QdotKt)\n",
    "\n",
    "    #------ MatMul soft and V ------#\n",
    "      # soft shape: (N, heads, query_len, key_len)\n",
    "      # values shape: (N, value_len, heads, head_dimension)\n",
    "    # we want\n",
    "      # (N, query_len, heads, head_dimension)\n",
    "      # after multiplying, flatten last two dimensions\n",
    "    out = torch.einsum('nhql,nlhd->nqhd', [soft,values]).reshape(\n",
    "      N, query_len, self.heads*self.head_dimension)\n",
    "\n",
    "    #------ Concatenate heads ------#\n",
    "    out = self.fully_connected_out(out)\n",
    "    \n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer Block\n",
    "Now that we have out attention mechanism we can construct the transformer block, which will be used to create both the encoder and decoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "  # transformer block architecture\n",
    "    # attention block -> add & normalize -> feed forward -> add & normalize\n",
    "\n",
    "  def __init__(self, embed_size, heads, dropout, forward_expansion):\n",
    "    super(TransformerBlock, self).__init__()\n",
    "    self.attention = SelfAttention(embed_size, heads)\n",
    "    self.norm1 = nn.LayerNorm(embed_size)\n",
    "    self.norm2 = nn.LayerNorm(embed_size)\n",
    "\n",
    "    self.feed_forward = nn.Sequential(\n",
    "      nn.Linear(embed_size, forward_expansion * embed_size),\n",
    "      nn.ReLU(),\n",
    "      nn.Linear(forward_expansion * embed_size, embed_size)\n",
    "    )\n",
    "\n",
    "    self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "  def forward(self, value, key, query, mask):\n",
    "    # attention\n",
    "    attention = self.attention(value, key, query, mask)\n",
    "\n",
    "    # add & norm\n",
    "    x = self.dropout(self.norm1(attention + query))\n",
    "\n",
    "    # feed forward\n",
    "    forward = self.feed_forward(x)\n",
    "\n",
    "    # add & norm\n",
    "    out = self.dropout(self.norm2(forward + x))\n",
    "\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder\n",
    "\n",
    "This encoder represents the left part of the diagram in Figure 1.\n",
    "\n",
    "Because the previous components we built will be used in the encoder, we have to pass through all of the hyperparameters.\n",
    "\n",
    "There is a new parameter called max_length, this is related to the positional enbedding. We have to tell the model what our max length of sentence is. It will vary based on the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "  def __init__(\n",
    "    self,\n",
    "    src_vocab_size,\n",
    "    embed_size,\n",
    "    num_layers,\n",
    "    heads,\n",
    "    device,\n",
    "    forward_expansion,\n",
    "    dropout,\n",
    "    max_length\n",
    "  ):\n",
    "\n",
    "    super(Encoder, self).__init__()\n",
    "    # define embeddings for input\n",
    "    self.embed_size = embed_size\n",
    "    self.device = device\n",
    "    self.word_embedding = nn.Embedding(src_vocab_size, embed_size)\n",
    "    self.positional_embedding = nn.Embedding(max_length, embed_size)\n",
    "\n",
    "    # define layers, which consists of a single transformer blocks\n",
    "    self.layers = nn.ModuleList(\n",
    "      [\n",
    "        TransformerBlock(\n",
    "          embed_size,\n",
    "          heads,\n",
    "          dropout=dropout,\n",
    "          forward_expansion=forward_expansion\n",
    "        )\n",
    "        for _ in range(num_layers)\n",
    "      ]\n",
    "    )\n",
    "    \n",
    "    self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "  def forward(self, x, mask):\n",
    "    # we get the number of examples and the sequence legnth\n",
    "    N, seq_len = x.shape\n",
    "\n",
    "    # we create a range of 0 to the sequence length for every example\n",
    "    positions = torch.arange(0, seq_len).expand(N, seq_len).to(self.device)\n",
    "\n",
    "    # we give the word embedding and the postions and the model will now know the positions of words\n",
    "    out = self.dropout(self.word_embedding(x) + self.positional_embedding(positions))\n",
    "\n",
    "    for layer in self.layers:\n",
    "      # in the encoder, all our v, k, q input vectors are the same\n",
    "      out = layer(out, out, out, mask)\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder\n",
    "\n",
    "First we will create the decoder block, without the embeddings or linear and softmax which are outside."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderBlock(nn.Module):\n",
    "  def __init__(\n",
    "    self,\n",
    "    embed_size,\n",
    "    heads,\n",
    "    forward_expansion,\n",
    "    dropout,\n",
    "    device\n",
    "  ):\n",
    "    super(DecoderBlock, self).__init__()\n",
    "    \n",
    "    self.attention = SelfAttention(embed_size, heads)\n",
    "    self.norm = nn.LayerNorm(embed_size)\n",
    "    self.transformer_block = TransformerBlock(\n",
    "      embed_size, heads, dropout, forward_expansion\n",
    "    )\n",
    "    self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "  def forward(self, x, value, key, src_mask, trg_mask):\n",
    "    # masked attention\n",
    "    attention = self.attention(x,x,x, trg_mask)\n",
    "\n",
    "    # add and norm\n",
    "    query = self.dropout(self.norm(attention + x))\n",
    "\n",
    "    # transformer block\n",
    "    out = self.transformer_block(value, key, query, src_mask)\n",
    "\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we can use the decoder block make the whole decoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "  def __init__(\n",
    "    self,\n",
    "    trg_vocab_size,\n",
    "    embed_size,\n",
    "    num_layers,\n",
    "    heads,\n",
    "    forward_expansion,\n",
    "    dropout,\n",
    "    device,\n",
    "    max_length\n",
    "  ):\n",
    "    super(Decoder, self).__init__()\n",
    "\n",
    "    # define embeddings \n",
    "    self.device = device\n",
    "    self.word_embedding = nn.Embedding(trg_vocab_size, embed_size)\n",
    "    self.positional_embedding = nn.Embedding(max_length, embed_size)\n",
    "\n",
    "    # create num_layers of the decoder block \n",
    "    self.layers = nn.ModuleList(\n",
    "      [\n",
    "        DecoderBlock(\n",
    "          embed_size, heads, forward_expansion, dropout, device\n",
    "        )\n",
    "        for _ in range(num_layers)\n",
    "      ]\n",
    "    )\n",
    "\n",
    "    # final linear layer\n",
    "    self.fully_connected_out = nn.Linear(embed_size, trg_vocab_size)\n",
    "\n",
    "    self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "  def forward(self, x, encoder_out, src_mask, trg_mask):\n",
    "    # we get the number of examples and the sequence legnth\n",
    "    N, seq_len = x.shape\n",
    "    \n",
    "    # we create a range of 0 to the sequence length for every example\n",
    "    positions = torch.arange(0, seq_len).expand(N, seq_len).to(self.device)\n",
    "\n",
    "    # we give the word embedding and the postions and the model will now know the positions of words\n",
    "    x = self.dropout((self.word_embedding(x) + self.positional_embedding(positions)))\n",
    "\n",
    "    # run the decoder block for all of the layers\n",
    "    for layer in self.layers:\n",
    "      x = layer(x, encoder_out, encoder_out, src_mask, trg_mask)\n",
    "\n",
    "    # run final linear layer\n",
    "    out = self.fully_connected_out(x)\n",
    "\n",
    "    # final softmax \n",
    "    # out = torch.softmax(out)\n",
    "\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer\n",
    "\n",
    "Now that we've constructed all of the components, we can put them together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "  def __init__(\n",
    "    self,\n",
    "    src_vocab_size,\n",
    "    trg_vocab_size,\n",
    "    src_pad_idx,\n",
    "    trg_pad_idx,\n",
    "    embed_size=256,\n",
    "    num_layers=6,\n",
    "    forward_expansion=4,\n",
    "    heads=8,\n",
    "    dropout=0,\n",
    "    # device='cuda',\n",
    "    device = 'cpu',\n",
    "    max_length=100\n",
    "  ):\n",
    "    super(Transformer, self).__init__()\n",
    "\n",
    "    # define encoder\n",
    "    self.encoder = Encoder(\n",
    "      src_vocab_size,\n",
    "      embed_size,\n",
    "      num_layers,\n",
    "      heads,\n",
    "      device,\n",
    "      forward_expansion,\n",
    "      dropout,\n",
    "      max_length\n",
    "    )\n",
    "\n",
    "    # define decoder\n",
    "    self.decoder = Decoder(\n",
    "      trg_vocab_size,\n",
    "      embed_size,\n",
    "      num_layers,\n",
    "      heads,\n",
    "      forward_expansion,\n",
    "      dropout,\n",
    "      device,\n",
    "      max_length\n",
    "    )\n",
    "\n",
    "    self.src_pad_idx = src_pad_idx\n",
    "    self.trg_pad_idx = trg_pad_idx\n",
    "    self.device = device\n",
    "\n",
    "  # define make src mask\n",
    "  def make_src_mask(self, src):\n",
    "    # we want the src_mask in the shape of (N, 1, 1, src_len)\n",
    "    # if src is the src pad index then it will be 1, if not it will be 0\n",
    "    src_mask = (src != self.src_pad_idx).unsqueeze(1).unsqueeze(2)\n",
    "    return src_mask.to(self.device)\n",
    "\n",
    "  # define make trg mask\n",
    "  def make_trg_mask(self, trg):\n",
    "    N, trg_len = trg.shape\n",
    "\n",
    "    # we want a lower triangular matrix \n",
    "    trg_mask = torch.tril(torch.ones((trg_len, trg_len))).expand(\n",
    "      N, 1, trg_len, trg_len\n",
    "    )\n",
    "\n",
    "    return trg_mask.to(self.device)\n",
    "\n",
    "  def forward(self, src, trg):\n",
    "    src_mask = self.make_src_mask(src)\n",
    "    trg_mask = self.make_trg_mask(trg)\n",
    "\n",
    "    enc_src = self.encoder(src, src_mask)\n",
    "\n",
    "    out = self.decoder(trg, enc_src, src_mask, trg_mask)\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing\n",
    "\n",
    "Now that we've created the transformer model, we can test it to see if it works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 7, 10])\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "  device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "  x = torch.tensor([[1,5,6,4,3,9,5,2,0], [1,8,7,3,4,5,6,7,2]]).to(device)\n",
    "\n",
    "  trg = torch.tensor([[1,7,4,3,5,9,2,0], [1,5,6,2,4,7,6,2]]).to(device)\n",
    "\n",
    "  src_pad_idx = 0\n",
    "  trg_pad_idx = 0\n",
    "  src_vocab_size = 10\n",
    "  trg_vocab_size = 10\n",
    "\n",
    "  model = Transformer(src_vocab_size, trg_vocab_size, src_pad_idx, trg_pad_idx).to(device)\n",
    "\n",
    "  out = model(x, trg[:, :-1])\n",
    "\n",
    "  print(out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Cases\n",
    "\n",
    "## Self attention forward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_size = 32\n",
    "heads = 4\n",
    "N = 10\n",
    "seq_len = 20\n",
    "\n",
    "values = torch.rand(N, seq_len, embed_size)\n",
    "keys = torch.rand(N, seq_len, embed_size)\n",
    "queries = torch.rand(N, seq_len, embed_size)\n",
    "mask = torch.ones(seq_len, seq_len)\n",
    "\n",
    "self_attn = SelfAttention(embed_size, heads)\n",
    "out = self_attn(values, keys, queries, mask)\n",
    "assert out.shape == (N, seq_len, embed_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TransformerBlock forward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_size = 32\n",
    "heads = 4\n",
    "N = 10\n",
    "seq_len = 20\n",
    "dropout = 0.2\n",
    "forward_expansion = 2\n",
    "\n",
    "value = torch.rand(N, seq_len, embed_size)\n",
    "key = torch.rand(N, seq_len, embed_size)\n",
    "query = torch.rand(N, seq_len, embed_size)\n",
    "mask = torch.ones(seq_len, seq_len)\n",
    "\n",
    "trans_block = TransformerBlock(embed_size, heads, dropout, forward_expansion)\n",
    "out = trans_block(value, key, query, mask)\n",
    "assert out.shape == (N, seq_len, embed_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Same tests but with Unittest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "...\n",
      "----------------------------------------------------------------------\n",
      "Ran 3 tests in 0.011s\n",
      "\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import unittest\n",
    "\n",
    "class TestSelfAttention(unittest.TestCase):\n",
    "\n",
    "    def test_forward_pass(self):\n",
    "        embed_size = 8\n",
    "        heads = 2\n",
    "        num_examples = 3\n",
    "        value_len = 5\n",
    "        key_len = 5\n",
    "        query_len = 5\n",
    "\n",
    "        values = torch.rand(num_examples, value_len, embed_size)\n",
    "        keys = torch.rand(num_examples, key_len, embed_size)\n",
    "        queries = torch.rand(num_examples, query_len, embed_size)\n",
    "        mask = None\n",
    "\n",
    "        attention_layer = SelfAttention(embed_size, heads)\n",
    "        output = attention_layer(values, keys, queries, mask)\n",
    "\n",
    "        self.assertEqual(output.size(), (num_examples, query_len, embed_size))\n",
    "\n",
    "class TestTransformerBlock(unittest.TestCase):\n",
    "\n",
    "    def test_forward_pass(self):\n",
    "        embed_size = 8\n",
    "        heads = 2\n",
    "        dropout = 0.1\n",
    "        forward_expansion = 4\n",
    "        num_examples = 3\n",
    "        value_len = 5\n",
    "        key_len = 5\n",
    "        query_len = 5\n",
    "\n",
    "        value = torch.rand(num_examples, value_len, embed_size)\n",
    "        key = torch.rand(num_examples, key_len, embed_size)\n",
    "        query = torch.rand(num_examples, query_len, embed_size)\n",
    "        mask = None\n",
    "\n",
    "        transformer_block = TransformerBlock(embed_size, heads, dropout, forward_expansion)\n",
    "        output = transformer_block(value, key, query, mask)\n",
    "\n",
    "        self.assertEqual(output.size(), (num_examples, query_len, embed_size))\n",
    "\n",
    "class TestEncoder(unittest.TestCase):\n",
    "\n",
    "    def test_forward_pass(self):\n",
    "        src_vocab_size = 100\n",
    "        embed_size = 16\n",
    "        num_layers = 2\n",
    "        heads = 4\n",
    "        forward_expansion = 4\n",
    "        dropout = 0.1\n",
    "        max_length = 10\n",
    "        num_examples = 3\n",
    "        seq_len = 6\n",
    "\n",
    "        x = torch.LongTensor(num_examples, seq_len).random_(0, src_vocab_size)\n",
    "        mask = np.triu(np.ones((seq_len, seq_len)), k=1).astype('bool')\n",
    "        mask = torch.from_numpy(mask).to(torch.bool)\n",
    "\n",
    "        encoder = Encoder(\n",
    "            src_vocab_size,\n",
    "            embed_size,\n",
    "            num_layers,\n",
    "            heads,\n",
    "            'cpu',\n",
    "            forward_expansion,\n",
    "            dropout,\n",
    "            max_length\n",
    "        )\n",
    "\n",
    "        output = encoder(x, mask)\n",
    "\n",
    "        self.assertEqual(output.size(), (num_examples, seq_len, embed_size))\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    unittest.main(argv=['first-arg-is-ignored'], exit=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run code below to train the model on IMDB dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# SentimentClassifier\n",
    "class SentimentClassifier(nn.Module):\n",
    "    def __init__(self, src_vocab_size, embed_size, num_layers, heads, device, forward_expansion, dropout, max_length, num_classes):\n",
    "        super(SentimentClassifier, self).__init__()\n",
    "\n",
    "        self.encoder = Encoder(src_vocab_size, embed_size, num_layers, heads, device, forward_expansion, dropout, max_length)\n",
    "        self.linear = nn.Linear(embed_size, num_classes)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        out = self.encoder(x, mask)\n",
    "        out = out.mean(dim=1)\n",
    "        out = self.linear(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchtext.data \n",
    "import torchtext.datasets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Define your hyperparameters\n",
    "src_vocab_size = 25_000\n",
    "embed_size = 512\n",
    "num_layers = 3\n",
    "heads = 8\n",
    "forward_expansion = 4\n",
    "dropout = 0.1\n",
    "max_length = 100\n",
    "num_classes = 2\n",
    "num_epochs = 5\n",
    "batch_size = 64\n",
    "lr = 0.0003\n",
    "\n",
    "TEXT = torchtext.data.Field(tokenize=\"spacy\", tokenizer_language=\"en_core_web_sm\", batch_first=True, lower=True, fix_length=max_length)\n",
    "LABEL = torchtext.data.LabelField(dtype=torch.float)\n",
    "\n",
    "train_data, test_data = torchtext.datasets.IMDB.splits(TEXT, LABEL)\n",
    "\n",
    "TEXT.build_vocab(train_data, max_size=src_vocab_size - 2)\n",
    "LABEL.build_vocab(train_data)\n",
    "\n",
    "train_iterator, test_iterator = torchtext.data.BucketIterator.splits(\n",
    "    (train_data, test_data),\n",
    "    batch_size=batch_size,\n",
    "    device=device,\n",
    "    sort_within_batch=True,\n",
    "    sort_key=lambda x: len(x.text),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentimentClassifier(src_vocab_size, embed_size, num_layers, heads, device, forward_expansion, dropout, max_length, num_classes).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, iterator, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    \n",
    "    for batch in iterator:\n",
    "        optimizer.zero_grad()\n",
    "        text, label = batch.text.to(device), batch.label.to(device)\n",
    "        \n",
    "        mask = (text != 1).unsqueeze(1).unsqueeze(2).to(device)\n",
    "        predictions = model(text, mask)\n",
    "\n",
    "        loss = criterion(predictions, label.long())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, criterion, device):\n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in iterator:\n",
    "            text, label = batch.text.to(device), batch.label.to(device)\n",
    "            \n",
    "            mask = torch.tensor(text != 1, dtype=torch.bool).to(device)\n",
    "            predictions = model(text, mask)\n",
    "\n",
    "            loss = criterion(predictions, label.long())\n",
    "            epoch_loss += loss.item()\n",
    "            \n",
    "    return epoch_loss / len(iterator)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[33], line 7\u001b[0m\n\u001b[1;32m      3\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39mlr)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[0;32m----> 7\u001b[0m     train_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_iterator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m     valid_loss \u001b[38;5;241m=\u001b[39m evaluate(model, test_iterator, criterion, device)\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] - Train Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Test Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvalid_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[30], line 13\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, iterator, criterion, optimizer, device)\u001b[0m\n\u001b[1;32m     10\u001b[0m predictions \u001b[38;5;241m=\u001b[39m model(text, mask)\n\u001b[1;32m     12\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(predictions, label\u001b[38;5;241m.\u001b[39mlong())\n\u001b[0;32m---> 13\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     15\u001b[0m epoch_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    201\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = train(model, train_iterator, criterion, optimizer, device)\n",
    "    valid_loss = evaluate(model, test_iterator, criterion, device)\n",
    "\n",
    "    print(f\"Epoch [{epoch + 1}/{num_epochs}] - Train Loss: {train_loss:.4f}, Test Loss: {valid_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
