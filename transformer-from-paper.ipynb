{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer from Attention Is All You Need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: Could not find a version that satisfies the requirement torchtext==0.10.0 (from versions: 0.1.1, 0.2.0, 0.2.1, 0.2.3, 0.3.1, 0.4.0, 0.5.0, 0.6.0, 0.12.0, 0.13.0, 0.13.1, 0.14.0, 0.14.1, 0.15.0, 0.15.1)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for torchtext==0.10.0\u001b[0m\u001b[31m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.1.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3.10 -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# import necessary packages\n",
    "import torch\n",
    "import torch.nn as nn "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention\n",
    "\n",
    "#### See section 3.2 "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll start by building the self attention module, this is what makes the transformer architecture so powerfull. It takes in an embedding size, and a number of heads. We split the embedding into however many heads there are, this is multi-head attention. \n",
    "\n",
    "(ex. if we have an embedding size 256 and heads is 8, we split it into 8x32 parts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "  def __init__(self, embed_size, heads):\n",
    "    super(SelfAttention, self).__init__()\n",
    "\n",
    "    assert (embed_size % heads == 0), 'Embed size not divisible by heads'\n",
    "\n",
    "    self.embed_size = embed_size\n",
    "    self.heads = heads\n",
    "    self.head_dimension = embed_size // heads\n",
    "\n",
    "    # define linear layers to send queries, keys, and values through\n",
    "    self.values = nn.Linear(self.head_dimension, self.head_dimension, bias=False)\n",
    "    self.keys = nn.Linear(self.head_dimension, self.head_dimension, bias=False)\n",
    "    self.queries = nn.Linear(self.head_dimension, self.head_dimension, bias=False)\n",
    "    \n",
    "    # concatenate heads after multi-head attention\n",
    "    self.fully_connected_out = nn.Linear(embed_size, embed_size)\n",
    "\n",
    "  def forward(self, values, keys, query, mask):\n",
    "    # num training examples to send in at one time\n",
    "    N = query.shape[0]\n",
    "\n",
    "    # these correspond to source sentence length and target sentence length\n",
    "    value_len = values.shape[1]\n",
    "    key_len = keys.shape[1]\n",
    "    query_len = query.shape[1]\n",
    "\n",
    "    # split embeddings into multiple heads\n",
    "    values = values.reshape(N, value_len, self.heads, self.head_dimension)\n",
    "    keys = keys.reshape(N, key_len, self.heads, self.head_dimension)\n",
    "    queries = query.reshape(N, query_len, self.heads, self.head_dimension)\n",
    "\n",
    "    # send through linear layers\n",
    "    values = self.values(values)\n",
    "    keys = self.keys(keys)\n",
    "    queries = self.queries(queries)\n",
    "\n",
    "    #------- MatMul Q and K(Transposed) ---------#\n",
    "      # queries shape: (N, query_len, heads, head_dimension)\n",
    "      # keys shape: (N, key_len, heads, head_dimension)\n",
    "    # we want\n",
    "      # QdotK shape: (N, heads, query_len, key_len)\n",
    "    QdotKt = torch.einsum('nqhd,nkhd->nhqk', [queries, keys])\n",
    "\n",
    "    #------------ Scale ------------#\n",
    "    # QdotKt = QdotKt / (self.embed_size ** (1/2))\n",
    "\n",
    "    #----- Mask (for decoder) ------#\n",
    "    # decoder requires masked multi-head attention\n",
    "    if mask is not None:\n",
    "      # closes elements above the diagonal so that the model can't see future values\n",
    "      QdotKt = QdotKt.masked_fill(mask == 0, float('-1e20'))\n",
    "\n",
    "    #---------- Softmax ------------#\n",
    "    soft = torch.softmax(QdotKt / (self.embed_size ** (1/2)), dim=3)\n",
    "    # attention = torch.softmax(QdotKt)\n",
    "\n",
    "    #------ MatMul soft and V ------#\n",
    "      # soft shape: (N, heads, query_len, key_len)\n",
    "      # values shape: (N, value_len, heads, head_dimension)\n",
    "    # we want\n",
    "      # (N, query_len, heads, head_dimension)\n",
    "      # after multiplying, flatten last two dimensions\n",
    "    out = torch.einsum('nhql,nlhd->nqhd', [soft,values]).reshape(\n",
    "      N, query_len, self.heads*self.head_dimension)\n",
    "\n",
    "    #------ Concatenate heads ------#\n",
    "    out = self.fully_connected_out(out)\n",
    "    \n",
    "    return out\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer Block\n",
    "Now that we have out attention mechanism we can construct the transformer block, which will be used to create both the encoder and decoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "  # transformer block architecture\n",
    "    # attention block -> add & normalize -> feed forward -> add & normalize\n",
    "\n",
    "  def __init__(self, embed_size, heads, dropout, forward_expansion):\n",
    "    super(TransformerBlock, self).__init__()\n",
    "    self.attention = SelfAttention(embed_size, heads)\n",
    "    self.norm1 = nn.LayerNorm(embed_size)\n",
    "    self.norm2 = nn.LayerNorm(embed_size)\n",
    "\n",
    "    self.feed_forward = nn.Sequential(\n",
    "      nn.Linear(embed_size, forward_expansion * embed_size),\n",
    "      nn.ReLU(),\n",
    "      nn.Linear(forward_expansion * embed_size, embed_size)\n",
    "    )\n",
    "\n",
    "    self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "  def forward(self, value, key, query, mask):\n",
    "    # attention\n",
    "    attention = self.attention(value, key, query, mask)\n",
    "\n",
    "    # add & norm\n",
    "    x = self.dropout(self.norm1(attention + query))\n",
    "\n",
    "    # feed forward\n",
    "    forward = self.feed_forward(x)\n",
    "\n",
    "    # add & norm\n",
    "    out = self.dropout(self.norm2(forward + x))\n",
    "\n",
    "    return out\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder\n",
    "\n",
    "This encoder represents the left part of the diagram in Figure 1.\n",
    "\n",
    "Because the previous components we built will be used in the encoder, we have to pass through all of the hyperparameters.\n",
    "\n",
    "There is a new parameter called max_length, this is related to the positional enbedding. We have to tell the model what our max length of sentence is. It will vary based on the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "  def __init__(\n",
    "    self,\n",
    "    src_vocab_size,\n",
    "    embed_size,\n",
    "    num_layers,\n",
    "    heads,\n",
    "    device,\n",
    "    forward_expansion,\n",
    "    dropout,\n",
    "    max_length\n",
    "  ):\n",
    "\n",
    "    super(Encoder, self).__init__()\n",
    "    # define embeddings for input\n",
    "    self.embed_size = embed_size\n",
    "    self.device = device\n",
    "    self.word_embedding = nn.Embedding(src_vocab_size, embed_size)\n",
    "    self.positional_embedding = nn.Embedding(max_length, embed_size)\n",
    "\n",
    "    # define layers, which consists of a single transformer blocks\n",
    "    self.layers = nn.ModuleList(\n",
    "      [\n",
    "        TransformerBlock(\n",
    "          embed_size,\n",
    "          heads,\n",
    "          dropout=dropout,\n",
    "          forward_expansion=forward_expansion\n",
    "        )\n",
    "        for _ in range(num_layers)\n",
    "      ]\n",
    "    )\n",
    "    \n",
    "    self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "  def forward(self, x, mask):\n",
    "    # we get the number of examples and the sequence legnth\n",
    "    N, seq_len = x.shape\n",
    "\n",
    "    # we create a range of 0 to the sequence length for every example\n",
    "    positions = torch.arange(0, seq_len).expand(N, seq_len).to(self.device)\n",
    "\n",
    "    # we give the word embedding and the postions and the model will now know the positions of words\n",
    "    out = self.dropout(self.word_embedding(x) + self.positional_embedding(positions))\n",
    "\n",
    "    for layer in self.layers:\n",
    "      # in the encoder, all our v, k, q input vectors are the same\n",
    "      out = layer(out, out, out, mask)\n",
    "\n",
    "    return out"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder\n",
    "\n",
    "First we will create the decoder block, without the embeddings or linear and softmax which are outside."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderBlock(nn.Module):\n",
    "  def __init__(\n",
    "    self,\n",
    "    embed_size,\n",
    "    heads,\n",
    "    forward_expansion,\n",
    "    dropout,\n",
    "    device\n",
    "  ):\n",
    "    super(DecoderBlock, self).__init__()\n",
    "    \n",
    "    self.attention = SelfAttention(embed_size, heads)\n",
    "    self.norm = nn.LayerNorm(embed_size)\n",
    "    self.transformer_block = TransformerBlock(\n",
    "      embed_size, heads, dropout, forward_expansion\n",
    "    )\n",
    "    self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "  def forward(self, x, value, key, src_mask, trg_mask):\n",
    "    # masked attention\n",
    "    attention = self.attention(x,x,x, trg_mask)\n",
    "\n",
    "    # add and norm\n",
    "    query = self.dropout(self.norm(attention + x))\n",
    "\n",
    "    # transformer block\n",
    "    out = self.transformer_block(value, key, query, src_mask)\n",
    "\n",
    "    return out\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we can use the decoder block make the whole decoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "  def __init__(\n",
    "    self,\n",
    "    trg_vocab_size,\n",
    "    embed_size,\n",
    "    num_layers,\n",
    "    heads,\n",
    "    forward_expansion,\n",
    "    dropout,\n",
    "    device,\n",
    "    max_length\n",
    "  ):\n",
    "    super(Decoder, self).__init__()\n",
    "\n",
    "    # define embeddings \n",
    "    self.device = device\n",
    "    self.word_embedding = nn.Embedding(trg_vocab_size, embed_size)\n",
    "    self.positional_embedding = nn.Embedding(max_length, embed_size)\n",
    "\n",
    "    # create num_layers of the decoder block \n",
    "    self.layers = nn.ModuleList(\n",
    "      [\n",
    "        DecoderBlock(\n",
    "          embed_size, heads, forward_expansion, dropout, device\n",
    "        )\n",
    "        for _ in range(num_layers)\n",
    "      ]\n",
    "    )\n",
    "\n",
    "    # final linear layer\n",
    "    self.fully_connected_out = nn.Linear(embed_size, trg_vocab_size)\n",
    "\n",
    "    self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "  def forward(self, x, encoder_out, src_mask, trg_mask):\n",
    "    # we get the number of examples and the sequence legnth\n",
    "    N, seq_len = x.shape\n",
    "    \n",
    "    # we create a range of 0 to the sequence length for every example\n",
    "    positions = torch.arange(0, seq_len).expand(N, seq_len).to(self.device)\n",
    "\n",
    "    # we give the word embedding and the postions and the model will now know the positions of words\n",
    "    x = self.dropout((self.word_embedding(x) + self.positional_embedding(positions)))\n",
    "\n",
    "    # run the decoder block for all of the layers\n",
    "    for layer in self.layers:\n",
    "      x = layer(x, encoder_out, encoder_out, src_mask, trg_mask)\n",
    "\n",
    "    # run final linear layer\n",
    "    out = self.fully_connected_out(x)\n",
    "\n",
    "    # final softmax \n",
    "    # out = torch.softmax(out)\n",
    "\n",
    "    return out\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer\n",
    "\n",
    "Now that we've constructed all of the components, we can put them together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "  def __init__(\n",
    "    self,\n",
    "    src_vocab_size,\n",
    "    trg_vocab_size,\n",
    "    src_pad_idx,\n",
    "    trg_pad_idx,\n",
    "    embed_size=256,\n",
    "    num_layers=6,\n",
    "    forward_expansion=4,\n",
    "    heads=8,\n",
    "    dropout=0,\n",
    "    # device='cuda',\n",
    "    device = 'cpu',\n",
    "    max_length=100\n",
    "  ):\n",
    "    super(Transformer, self).__init__()\n",
    "\n",
    "    # define encoder\n",
    "    self.encoder = Encoder(\n",
    "      src_vocab_size,\n",
    "      embed_size,\n",
    "      num_layers,\n",
    "      heads,\n",
    "      device,\n",
    "      forward_expansion,\n",
    "      dropout,\n",
    "      max_length\n",
    "    )\n",
    "\n",
    "    # define decoder\n",
    "    self.decoder = Decoder(\n",
    "      trg_vocab_size,\n",
    "      embed_size,\n",
    "      num_layers,\n",
    "      heads,\n",
    "      forward_expansion,\n",
    "      dropout,\n",
    "      device,\n",
    "      max_length\n",
    "    )\n",
    "\n",
    "    self.src_pad_idx = src_pad_idx\n",
    "    self.trg_pad_idx = trg_pad_idx\n",
    "    self.device = device\n",
    "\n",
    "  # define make src mask\n",
    "  def make_src_mask(self, src):\n",
    "    # we want the src_mask in the shape of (N, 1, 1, src_len)\n",
    "    # if src is the src pad index then it will be 1, if not it will be 0\n",
    "    src_mask = (src != self.src_pad_idx).unsqueeze(1).unsqueeze(2)\n",
    "    return src_mask.to(self.device)\n",
    "\n",
    "  # define make trg mask\n",
    "  def make_trg_mask(self, trg):\n",
    "    N, trg_len = trg.shape\n",
    "\n",
    "    # we want a lower triangular matrix \n",
    "    trg_mask = torch.tril(torch.ones((trg_len, trg_len))).expand(\n",
    "      N, 1, trg_len, trg_len\n",
    "    )\n",
    "\n",
    "    return trg_mask.to(self.device)\n",
    "\n",
    "  def forward(self, src, trg):\n",
    "    src_mask = self.make_src_mask(src)\n",
    "    trg_mask = self.make_trg_mask(trg)\n",
    "\n",
    "    enc_src = self.encoder(src, src_mask)\n",
    "\n",
    "    out = self.decoder(trg, enc_src, src_mask, trg_mask)\n",
    "\n",
    "    return out"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing\n",
    "\n",
    "Now that we've created the transformer model, we can test it to see if it works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 7, 10])\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "  device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "  x = torch.tensor([[1,5,6,4,3,9,5,2,0], [1,8,7,3,4,5,6,7,2]]).to(device)\n",
    "\n",
    "  trg = torch.tensor([[1,7,4,3,5,9,2,0], [1,5,6,2,4,7,6,2]]).to(device)\n",
    "\n",
    "  src_pad_idx = 0\n",
    "  trg_pad_idx = 0\n",
    "  src_vocab_size = 10\n",
    "  trg_vocab_size = 10\n",
    "\n",
    "  model = Transformer(src_vocab_size, trg_vocab_size, src_pad_idx, trg_pad_idx).to(device)\n",
    "\n",
    "  out = model(x, trg[:, :-1])\n",
    "\n",
    "  print(out.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
