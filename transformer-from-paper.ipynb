{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer from Attention Is All You Need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary packages\n",
    "import torch\n",
    "import torch.nn as nn "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention\n",
    "\n",
    "#### See section 3.2 "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll start by building the self attention module, this is what makes the transformer architecture so powerfull. It takes in an embedding size, and a number of heads. We split the embedding into however many heads there are, this is multi-head attention. \n",
    "\n",
    "(ex. if we have an embedding size 256 and heads is 8, we split it into 8x32 parts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "  def __init__(self, embed_size, heads):\n",
    "    super(SelfAttention, self).__init__()\n",
    "\n",
    "    assert (embed_size % heads == 0), 'Embed size not divisible by heads'\n",
    "\n",
    "    self.embed_size = embed_size\n",
    "    self.heads = heads\n",
    "    self.head_dimension = embed_size // heads\n",
    "\n",
    "    # define linear layers to send queries, keys, and values through\n",
    "    self.values = nn.Linear(self.head_dimension, self.head_dimension, bias=False)\n",
    "    self.keys = nn.Linear(self.head_dimension, self.head_dimension, bias=False)\n",
    "    self.queries = nn.Linear(self.head_dimension, self.head_dimension, bias=False)\n",
    "    \n",
    "    # concatenate heads after multi-head attention\n",
    "    self.fully_connected_out = nn.Linear(embed_size, embed_size)\n",
    "\n",
    "  def forward(self, values, keys, query, mask):\n",
    "    # num training examples to send in at one time\n",
    "    N = query.shape[0]\n",
    "\n",
    "    # these correspond to source sentence length and target sentence length\n",
    "    value_len = values.shape[1]\n",
    "    key_len = keys.shape[1]\n",
    "    query_len = query.shape[1]\n",
    "\n",
    "    # split embeddings into multiple heads\n",
    "    values = values.reshape(N, value_len, self.heads, self.head_dimension)\n",
    "    keys = keys.reshape(N, key_len, self.heads, self.head_dimension)\n",
    "    queries = query.reshape(N, query_len, self.heads, self.head_dimension)\n",
    "\n",
    "    # send through linear layers\n",
    "    values = self.values(values)\n",
    "    keys = self.keys(keys)\n",
    "    queries = self.queries(queries)\n",
    "\n",
    "    #------- MatMul Q and K(Transposed) ---------#\n",
    "      # queries shape: (N, query_len, heads, head_dimension)\n",
    "      # keys shape: (N, key_len, heads, head_dimension)\n",
    "    # we want\n",
    "      # QdotK shape: (N, heads, query_len, key_len)\n",
    "    QdotKt = torch.einsum('nqhd,nkhd->nhqk', [queries, keys])\n",
    "\n",
    "    #------------ Scale ------------#\n",
    "    # QdotKt = QdotKt / (self.embed_size ** (1/2))\n",
    "\n",
    "    #----- Mask (for decoder) ------#\n",
    "    # decoder requires masked multi-head attention\n",
    "    if mask is not None:\n",
    "      # closes elements above the diagonal so that the model can't see future values\n",
    "      QdotKt = QdotKt.masked_fill(mask == 0, float('-1e20'))\n",
    "\n",
    "    #---------- Softmax ------------#\n",
    "    soft = torch.softmax(QdotKt / (self.embed_size ** (1/2)), dim=3)\n",
    "    # attention = torch.softmax(QdotKt)\n",
    "\n",
    "    #------ MatMul soft and V ------#\n",
    "      # soft shape: (N, heads, query_len, key_len)\n",
    "      # values shape: (N, value_len, heads, head_dimension)\n",
    "    # we want\n",
    "      # (N, query_len, heads, head_dimension)\n",
    "      # after multiplying, flatten last two dimensions\n",
    "    out = torch.einsum('nhql,nlhd->nqhd', [soft,values]).reshape(\n",
    "      N, query_len, self.heads*self.head_dimension)\n",
    "\n",
    "    #------ Concatenate heads ------#\n",
    "    out = self.fully_connected_out(out)\n",
    "    \n",
    "    return out\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer Block\n",
    "Now that we have out attention mechanism we can construct the transformer block, which will be used to create both the encoder and decoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "  # transformer block architecture\n",
    "    # attention block -> add & normalize -> feed forward -> add & normalize\n",
    "\n",
    "  def __init__(self, embed_size, heads, dropout, forward_expansion):\n",
    "    super(TransformerBlock, self).__init__()\n",
    "    self.attention = SelfAttention(embed_size, heads)\n",
    "    self.norm1 = nn.LayerNorm(embed_size)\n",
    "    self.norm2 = nn.LayerNorm(embed_size)\n",
    "\n",
    "    self.feed_forward = nn.Sequential(\n",
    "      nn.Linear(embed_size, forward_expansion * embed_size),\n",
    "      nn.ReLU(),\n",
    "      nn.Linear(forward_expansion * embed_size, embed_size)\n",
    "    )\n",
    "\n",
    "    self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "  def forward(self, value, key, query, mask):\n",
    "    # attention\n",
    "    attention = self.attention(value, key, query, mask)\n",
    "\n",
    "    # add & norm\n",
    "    x = self.dropout(self.norm1(attention + query))\n",
    "\n",
    "    # feed forward\n",
    "    forward = self.feed_forward(x)\n",
    "\n",
    "    # add & norm\n",
    "    out = self.dropout(self.norm2(forward + x))\n",
    "\n",
    "    return out\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder\n",
    "\n",
    "This encoder represents the left part of the diagram in Figure 1.\n",
    "\n",
    "Because the previous components we built will be used in the encoder, we have to pass through all of the hyperparameters.\n",
    "\n",
    "There is a new parameter called max_length, this is related to the positional enbedding. We have to tell the model what our max length of sentence is. It will vary based on the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "  def __init__(\n",
    "    self,\n",
    "    src_vocab_size,\n",
    "    embed_size,\n",
    "    num_layers,\n",
    "    heads,\n",
    "    device,\n",
    "    forward_expansion,\n",
    "    dropout,\n",
    "    max_length\n",
    "  ):\n",
    "\n",
    "    super(Encoder, self).__init__()\n",
    "    # define embeddings for input\n",
    "    self.embed_size = embed_size\n",
    "    self.device = device\n",
    "    self.word_embedding = nn.Embedding(src_vocab_size, embed_size)\n",
    "    self.positional_embedding = nn.Embedding(max_length, embed_size)\n",
    "\n",
    "    # define layers, which consists of a single transformer blocks\n",
    "    self.layers = nn.ModuleList(\n",
    "      [\n",
    "        TransformerBlock(\n",
    "          embed_size,\n",
    "          heads,\n",
    "          dropout=dropout,\n",
    "          forward_expansion=forward_expansion\n",
    "        )\n",
    "        for _ in range(num_layers)\n",
    "      ]\n",
    "    )\n",
    "    \n",
    "    self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "  def forward(self, x, mask):\n",
    "    # we get the number of examples and the sequence legnth\n",
    "    N, seq_len = x.shape\n",
    "\n",
    "    # we create a range of 0 to the sequence length for every example\n",
    "    positions = torch.arange(0, seq_len).expand(N, seq_len).to(self.device)\n",
    "\n",
    "    # we give the word embedding and the postions and the model will now know the positions of words\n",
    "    out = self.dropout(self.word_embedding(x) + self.positional_embedding(positions))\n",
    "\n",
    "    for layer in self.layers:\n",
    "      # in the encoder, all our v, k, q input vectors are the same\n",
    "      out = layer(out, out, out, mask)\n",
    "\n",
    "    return out"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder\n",
    "\n",
    "First we will create the decoder block, without the embeddings or linear and softmax which are outside."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderBlock(nn.Module):\n",
    "  def __init__(\n",
    "    self,\n",
    "    embed_size,\n",
    "    heads,\n",
    "    forward_expansion,\n",
    "    dropout,\n",
    "    device\n",
    "  ):\n",
    "    super(DecoderBlock, self).__init__()\n",
    "    \n",
    "    self.attention = SelfAttention(embed_size, heads)\n",
    "    self.norm = nn.LayerNorm(embed_size)\n",
    "    self.transformer_block = TransformerBlock(\n",
    "      embed_size, heads, dropout, forward_expansion\n",
    "    )\n",
    "    self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "  def forward(self, x, value, key, src_mask, trg_mask):\n",
    "    # masked attention\n",
    "    attention = self.attention(x,x,x, trg_mask)\n",
    "\n",
    "    # add and norm\n",
    "    query = self.dropout(self.norm(attention + x))\n",
    "\n",
    "    # transformer block\n",
    "    out = self.transformer_block(value, key, query, src_mask)\n",
    "\n",
    "    return out\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we can use the decoder block make the whole decoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "  def __init__(\n",
    "    self,\n",
    "    trg_vocab_size,\n",
    "    embed_size,\n",
    "    num_layers,\n",
    "    heads,\n",
    "    forward_expansion,\n",
    "    dropout,\n",
    "    device,\n",
    "    max_length\n",
    "  ):\n",
    "    super(Decoder, self).__init__()\n",
    "\n",
    "    # define embeddings \n",
    "    self.device = device\n",
    "    self.word_embedding = nn.Embedding(trg_vocab_size, embed_size)\n",
    "    self.positional_embedding = nn.Embedding(max_length, embed_size)\n",
    "\n",
    "    # create num_layers of the decoder block \n",
    "    self.layers = nn.ModuleList(\n",
    "      [\n",
    "        DecoderBlock(\n",
    "          embed_size, heads, forward_expansion, dropout, device\n",
    "        )\n",
    "        for _ in range(num_layers)\n",
    "      ]\n",
    "    )\n",
    "\n",
    "    # final linear layer\n",
    "    self.fully_connected_out = nn.Linear(embed_size, trg_vocab_size)\n",
    "\n",
    "    self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "  def forward(self, x, encoder_out, src_mask, trg_mask):\n",
    "    # we get the number of examples and the sequence legnth\n",
    "    N, seq_len = x.shape\n",
    "    \n",
    "    # we create a range of 0 to the sequence length for every example\n",
    "    positions = torch.arange(0, seq_len).expand(N, seq_len).to(self.device)\n",
    "\n",
    "    # we give the word embedding and the postions and the model will now know the positions of words\n",
    "    x = self.dropout((self.word_embedding(x) + self.positional_embedding(positions)))\n",
    "\n",
    "    # run the decoder block for all of the layers\n",
    "    for layer in self.layers:\n",
    "      x = layer(x, encoder_out, encoder_out, src_mask, trg_mask)\n",
    "\n",
    "    # run final linear layer\n",
    "    out = self.fully_connected_out(x)\n",
    "\n",
    "    # final softmax \n",
    "    # out = torch.softmax(out)\n",
    "\n",
    "    return out\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer\n",
    "\n",
    "Now that we've constructed all of the components, we can put them together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "  def __init__(\n",
    "    self,\n",
    "    src_vocab_size,\n",
    "    trg_vocab_size,\n",
    "    src_pad_idx,\n",
    "    trg_pad_idx,\n",
    "    embed_size=256,\n",
    "    num_layers=6,\n",
    "    forward_expansion=4,\n",
    "    heads=8,\n",
    "    dropout=0,\n",
    "    # device='cuda',\n",
    "    device = 'cpu',\n",
    "    max_length=100\n",
    "  ):\n",
    "    super(Transformer, self).__init__()\n",
    "\n",
    "    # define encoder\n",
    "    self.encoder = Encoder(\n",
    "      src_vocab_size,\n",
    "      embed_size,\n",
    "      num_layers,\n",
    "      heads,\n",
    "      device,\n",
    "      forward_expansion,\n",
    "      dropout,\n",
    "      max_length\n",
    "    )\n",
    "\n",
    "    # define decoder\n",
    "    self.decoder = Decoder(\n",
    "      trg_vocab_size,\n",
    "      embed_size,\n",
    "      num_layers,\n",
    "      heads,\n",
    "      forward_expansion,\n",
    "      dropout,\n",
    "      device,\n",
    "      max_length\n",
    "    )\n",
    "\n",
    "    self.src_pad_idx = src_pad_idx\n",
    "    self.trg_pad_idx = trg_pad_idx\n",
    "    self.device = device\n",
    "\n",
    "  # define make src mask\n",
    "  def make_src_mask(self, src):\n",
    "    # we want the src_mask in the shape of (N, 1, 1, src_len)\n",
    "    # if src is the src pad index then it will be 1, if not it will be 0\n",
    "    src_mask = (src != self.src_pad_idx).unsqueeze(1).unsqueeze(2)\n",
    "    return src_mask.to(self.device)\n",
    "\n",
    "  # define make trg mask\n",
    "  def make_trg_mask(self, trg):\n",
    "    N, trg_len = trg.shape\n",
    "\n",
    "    # we want a lower triangular matrix \n",
    "    trg_mask = torch.tril(torch.ones((trg_len, trg_len))).expand(\n",
    "      N, 1, trg_len, trg_len\n",
    "    )\n",
    "\n",
    "    return trg_mask.to(self.device)\n",
    "\n",
    "  def forward(self, src, trg):\n",
    "    src_mask = self.make_src_mask(src)\n",
    "    trg_mask = self.make_trg_mask(trg)\n",
    "\n",
    "    enc_src = self.encoder(src, src_mask)\n",
    "\n",
    "    out = self.decoder(trg, enc_src, src_mask, trg_mask)\n",
    "\n",
    "    return out"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing\n",
    "\n",
    "Now that we've created the transformer model, we can test it to see if it works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 7, 10])\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "  device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "  x = torch.tensor([[1,5,6,4,3,9,5,2,0], [1,8,7,3,4,5,6,7,2]]).to(device)\n",
    "\n",
    "  trg = torch.tensor([[1,7,4,3,5,9,2,0], [1,5,6,2,4,7,6,2]]).to(device)\n",
    "\n",
    "  src_pad_idx = 0\n",
    "  trg_pad_idx = 0\n",
    "  src_vocab_size = 10\n",
    "  trg_vocab_size = 10\n",
    "\n",
    "  model = Transformer(src_vocab_size, trg_vocab_size, src_pad_idx, trg_pad_idx).to(device)\n",
    "\n",
    "  out = model(x, trg[:, :-1])\n",
    "\n",
    "  print(out.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Cases\n",
    "\n",
    "## Self attention forward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_size = 32\n",
    "heads = 4\n",
    "N = 10\n",
    "seq_len = 20\n",
    "\n",
    "values = torch.rand(N, seq_len, embed_size)\n",
    "keys = torch.rand(N, seq_len, embed_size)\n",
    "queries = torch.rand(N, seq_len, embed_size)\n",
    "mask = torch.ones(seq_len, seq_len)\n",
    "\n",
    "self_attn = SelfAttention(embed_size, heads)\n",
    "out = self_attn(values, keys, queries, mask)\n",
    "assert out.shape == (N, seq_len, embed_size)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TransformerBlock forward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_size = 32\n",
    "heads = 4\n",
    "N = 10\n",
    "seq_len = 20\n",
    "dropout = 0.2\n",
    "forward_expansion = 2\n",
    "\n",
    "value = torch.rand(N, seq_len, embed_size)\n",
    "key = torch.rand(N, seq_len, embed_size)\n",
    "query = torch.rand(N, seq_len, embed_size)\n",
    "mask = torch.ones(seq_len, seq_len)\n",
    "\n",
    "trans_block = TransformerBlock(embed_size, heads, dropout, forward_expansion)\n",
    "out = trans_block(value, key, query, mask)\n",
    "assert out.shape == (N, seq_len, embed_size)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Same tests but with Unittest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E\n",
      "======================================================================\n",
      "ERROR: /Users/liampropst/Library/Jupyter/runtime/kernel-ccc7a663-cc14-4cbe-ba0c-0a2b72de572d (unittest.loader._FailedTest)\n",
      "----------------------------------------------------------------------\n",
      "AttributeError: module '__main__' has no attribute '/Users/liampropst/Library/Jupyter/runtime/kernel-ccc7a663-cc14-4cbe-ba0c-0a2b72de572d'\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 1 test in 0.000s\n",
      "\n",
      "FAILED (errors=1)\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "True",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/liampropst/Library/Python/3.10/lib/python/site-packages/IPython/core/interactiveshell.py:3441: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import unittest\n",
    "\n",
    "class TestSelfAttention(unittest.TestCase):\n",
    "\n",
    "    def test_forward_pass(self):\n",
    "        embed_size = 8\n",
    "        heads = 2\n",
    "        num_examples = 3\n",
    "        value_len = 5\n",
    "        key_len = 5\n",
    "        query_len = 5\n",
    "\n",
    "        values = torch.rand(num_examples, value_len, embed_size)\n",
    "        keys = torch.rand(num_examples, key_len, embed_size)\n",
    "        queries = torch.rand(num_examples, query_len, embed_size)\n",
    "        mask = None\n",
    "\n",
    "        attention_layer = SelfAttention(embed_size, heads)\n",
    "        output = attention_layer(values, keys, queries, mask)\n",
    "\n",
    "        self.assertEqual(output.size(), (num_examples, query_len, embed_size))\n",
    "\n",
    "class TestTransformerBlock(unittest.TestCase):\n",
    "\n",
    "    def test_forward_pass(self):\n",
    "        embed_size = 8\n",
    "        heads = 2\n",
    "        dropout = 0.1\n",
    "        forward_expansion = 4\n",
    "        num_examples = 3\n",
    "        value_len = 5\n",
    "        key_len = 5\n",
    "        query_len = 5\n",
    "\n",
    "        value = torch.rand(num_examples, value_len, embed_size)\n",
    "        key = torch.rand(num_examples, key_len, embed_size)\n",
    "        query = torch.rand(num_examples, query_len, embed_size)\n",
    "        mask = None\n",
    "\n",
    "        transformer_block = TransformerBlock(embed_size, heads, dropout, forward_expansion)\n",
    "        output = transformer_block(value, key, query, mask)\n",
    "\n",
    "        self.assertEqual(output.size(), (num_examples, query_len, embed_size))\n",
    "\n",
    "class TestEncoder(unittest.TestCase):\n",
    "\n",
    "    def test_forward_pass(self):\n",
    "        src_vocab_size = 100\n",
    "        embed_size = 16\n",
    "        num_layers = 2\n",
    "        heads = 4\n",
    "        forward_expansion = 4\n",
    "        dropout = 0.1\n",
    "        max_length = 10\n",
    "        num_examples = 3\n",
    "        seq_len = 6\n",
    "\n",
    "        x = torch.LongTensor(num_examples, seq_len).random_(0, src_vocab_size)\n",
    "        mask = np.triu(np.ones((seq_len, seq_len)), k=1).astype('bool')\n",
    "        mask = torch.from_numpy(mask).to(torch.bool)\n",
    "\n",
    "        encoder = Encoder(\n",
    "            src_vocab_size,\n",
    "            embed_size,\n",
    "            num_layers,\n",
    "            heads,\n",
    "            'cpu',\n",
    "            forward_expansion,\n",
    "            dropout,\n",
    "            max_length\n",
    "        )\n",
    "\n",
    "        output = encoder(x, mask)\n",
    "\n",
    "        self.assertEqual(output.size(), (num_examples, seq_len, embed_size))\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    unittest.main()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
