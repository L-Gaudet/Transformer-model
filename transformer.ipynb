{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer from \"Attention Is All You Need\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.0\n"
     ]
    }
   ],
   "source": [
    "# import all necessary libraries\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import math, copy, re, random\n",
    "import warnings\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import torchtext\n",
    "import matplotlib.pyplot as plt\n",
    "warnings.simplefilter('ignore')\n",
    "print(torch.__version__)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Basic Components\n",
    "\n",
    "### Embeddings\n",
    "Each word needs to be converted to an embedding vector that the model can work with. The paper specified that each embeddings will produce a vector with 512 dimensions.\n",
    "It does this using an encoder that is composed of 6 identical layers. Each layer has two sub-layers: multi-head self-attention mechanism and postion-wise feed-forward network. Following by layer normalization. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embedding(nn.Module):\n",
    "  def __init__(self, vocab_size, embed_dim):\n",
    "    super(Embedding, self).__init__()\n",
    "    self.embed = nn.Embedding(vocab_size, embed_dim)\n",
    "\n",
    "  def forward(self, x):\n",
    "    out = self.embed(x)\n",
    "    return out\n",
    "  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Positional Encoding\n",
    "\n",
    "Next, generate possitional encoding. That is, we need to know:\n",
    "  * what the word means\n",
    "  * the position of the word in the sentence\n",
    "\n",
    "The paper outlines the following functions to create positional encoding:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEmbedding(nn.Module):\n",
    "  def __init__(self, max_seq_len, embed_model_dim):\n",
    "    super(PositionalEmbedding, self).__init__()\n",
    "    self.embed_dim = embed_model_dim\n",
    "\n",
    "    pe = torch.zeros(max_seq_len, self.embed_dim)\n",
    "    for pos in range(max_seq_len):\n",
    "      for i in range(0, self.embed_dim, 2):\n",
    "        pe[pos, i] = math.sin(pos/(10000**((2*i)/self.embed_dim)))\n",
    "        pe[pos,i+1] = math.cos(pos/(10000**((2*(i+1))/self.embed_dim)))\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = x * math.sqrt(self.embed_dim)\n",
    "    seq_len = x.size(1)\n",
    "    x = x + torch.autograd.Variable(self.pe[:,:seq_len], requires_grad=False)\n",
    "    return x"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Self Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, k, heads=4, mask=False):\n",
    "        super().__init()\n",
    "\n",
    "        assert k % heads == 0\n",
    "\n",
    "        self.k, self.head = k, heads\n",
    "\n",
    "        # these compute the queries, keys and values for all heads\n",
    "        self.tokeys = nn.Linear(k, k, bias=False)\n",
    "        self.toqueries = nn.Linear(k, k, bias=False)\n",
    "        self.tovalues = nn.Linear(k, k, bias=False)\n",
    "\n",
    "        # this will contatenate the heads after the multi-head attention\n",
    "        self.unifyheads = nn.Linear(k, k)\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, t, k = x.size()\n",
    "        h = self.heads\n",
    "\n",
    "        queries = self.toqueries(x)\n",
    "        keys = self.tokeys(x)\n",
    "        values = self.tovalues(x)\n",
    "\n",
    "        s = k//h\n",
    "\n",
    "        keys = keys.view(b,t,h,s)\n",
    "        queries = queries.view(b,t,h,s)\n",
    "        values = values.view(b,t,h,s)\n",
    "\n",
    "        # fold heads into the batch dimension\n",
    "        keys = keys.transpose(1, 2).contiguous().view(b * h, t, s)\n",
    "        queries = queries.transpose(1, 2).contiguous().view(b * h, t, s)\n",
    "        values = values.transpose(1, 2).contiguous().view(b * h, t, s)\n",
    "\n",
    "        # get dot product of queries and keys, and scale\n",
    "        dot  = torch.bmm(queries, keys.transpose(1,2))\n",
    "\n",
    "        # scaled the dot product\n",
    "        dot = dot/(k**(1/2))\n",
    "\n",
    "        # normalize\n",
    "        dot = F.softmax(dot, dim=2)\n",
    "\n",
    "        # apply the self attention to the values\n",
    "        out = torch.bmm(dot, values).view(b,h,t,s)\n",
    "\n",
    "        # swap h, t back and unify heads\n",
    "        out = out.transpose(1,2).contiguous().view(b,t.s*h)\n",
    "\n",
    "        return self.unifyheads(out)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
